import type { VercelRequest, VercelResponse } from '@vercel/node';

/**
 * Vercel Serverless Function for AI Analysis
 * Converts Express.js backend logic to Vercel Functions
 */

// OpenAI API integration
interface OpenAIResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface AnalysisResult {
  expertiseScore: number;
  personalizedInsight: string;
  businessHint: string;
  marketOpportunity: string;
  successProbability: string;
  keyStrengths: string[];
  nextStepTeaser: string;
  exclusiveValue: string;
  urgencyFactor: string;
}

interface UserAnswers {
  [key: string]: any;
}

// Cost calculation function (from backend)
function calculateCost(promptTokens: number, completionTokens: number, model: string = 'gpt-4o'): number {
  const rates = {
    'gpt-4o': { prompt: 0.00001500, completion: 0.00006000 },
    'gpt-4o-mini': { prompt: 0.00000150, completion: 0.00000600 }
  };
  const rate = rates[model as keyof typeof rates] || rates['gpt-4o-mini'];
  return (promptTokens * rate.prompt) + (completionTokens * rate.completion);
}

// Generate analysis prompt
function generateAnalysisPrompt(answers: UserAnswers): string {
  const expertise = answers.expertise_field || 'ì¼ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤';
  const experience = answers.experience_years || '5ë…„';
  const name = answers.basic_name || 'ê³ ê°';
  
  return `ë‹¹ì‹ ì€ 40-50ëŒ€ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ IT ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¶„ì„ ëŒ€ìƒ:
- ì´ë¦„: ${name}
- ì „ë¬¸ ë¶„ì•¼: ${expertise}
- ê²½í—˜: ${experience}
- ê¸°íƒ€ ì •ë³´: ${JSON.stringify(answers, null, 2)}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "expertiseScore": ì ìˆ˜(0-100),
  "personalizedInsight": "${name}ë‹˜ì˜ ${expertise} ë¶„ì•¼ ê²½í—˜ì„ í™œìš©í•œ ê°œì¸í™”ëœ í†µì°° (120ì)",
  "businessHint": "${expertise} Ã— AI ìœµí•© ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ (90ì)",
  "marketOpportunity": "${expertise} AI ì‹œì¥ ê¸°íšŒ ë° ì„±ì¥ ì „ë§ (90ì)",
  "successProbability": "ì„±ê³µ í™•ë¥ % (ìœ ì‚¬ ë°°ê²½ ì „ë¬¸ê°€ ê¸°ì¤€)",
  "keyStrengths": ["ê°•ì 1", "ê°•ì 2", "ê°•ì 3", "ê°•ì 4"],
  "nextStepTeaser": "êµ¬ì²´ì  ê¸°ìˆ ìŠ¤íƒ/íŒŒíŠ¸ë„ˆì‚¬/ë¡œë“œë§µ ì •ë³´ í•„ìš”ì„± (110ì)",
  "exclusiveValue": "ExpertTech ë…ì  ìì‚°/ë„¤íŠ¸ì›Œí¬ ê°€ì¹˜ (90ì)",
  "urgencyFactor": "ì§€ê¸ˆ ì‹œì‘í•´ì•¼ í•˜ëŠ” ëª…í™•í•œ ì´ìœ  (90ì)"
}

**ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”**: 
- ìˆœìˆ˜í•œ JSON ê°ì²´ë§Œ ë°˜í™˜
- ì½”ë“œë¸”ë¡(\`\`\`) ê¸ˆì§€
- ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€
- { ë¡œ ì‹œì‘í•´ì„œ } ë¡œ ëë‚˜ëŠ” JSONë§Œ ë°˜í™˜
- "I'm sorry" ê°™ì€ í…ìŠ¤íŠ¸ ì ˆëŒ€ ê¸ˆì§€`;
}

// OpenAI API call function
async function callOpenAI(prompt: string, model: string = 'gpt-4o'): Promise<string> {
  const apiKey = process.env.OPENAI_API_KEY;
  
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY environment variable is not set');
  }

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: model,
      messages: [
        {
          role: 'system',
          content: 'ë‹¹ì‹ ì€ 40-50ëŒ€ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ IT ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì „ëµì ì´ê³  ê°œì¸í™”ëœ ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ë†’ì€ í’ˆì§ˆì˜ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë°©í–¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.7,
      max_tokens: model === 'gpt-4o' ? 2000 : 1500,
      response_format: { type: "json_object" }
    })
  });

  if (!response.ok) {
    const errorData = await response.text();
    throw new Error(`OpenAI API error (${model}): ${response.status} ${response.statusText} - ${errorData}`);
  }

  const data: OpenAIResponse = await response.json();
  
  // Log usage and cost
  const { prompt_tokens, completion_tokens, total_tokens } = data.usage;
  const cost = calculateCost(prompt_tokens, completion_tokens, model);
  
  console.log(`ğŸ’° ${model} Usage: ${total_tokens} tokens, ~$${cost.toFixed(4)}`);
  console.log(`âœ… ${model} Response: ${Date.now()}ms, Tokens: ${total_tokens}`);

  return data.choices[0].message.content;
}

// Enhanced JSON parsing with fallback
function parseAnalysisResult(llmResponse: string, answers: UserAnswers): AnalysisResult {
  try {
    // 1ë‹¨ê³„: ì‘ë‹µ ì •ë¦¬
    let cleanedResponse = llmResponse.trim();
    
    // 2ë‹¨ê³„: ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
    const unwantedPatterns = [
      /^I'm sorry[^{]*/i,
      /^Here is[^{]*/i,
      /^ë‹¤ìŒì€[^{]*/,
      /^ì•„ë˜ëŠ”[^{]*/,
      /^JSON ì‘ë‹µ[^{]*/,
      /^ë¶„ì„ ê²°ê³¼[^{]*/,
      /```json\s*/g,
      /```\s*/g,
      /\s*```$/g
    ];
    
    for (const pattern of unwantedPatterns) {
      cleanedResponse = cleanedResponse.replace(pattern, '');
    }
    
    // 3ë‹¨ê³„: JSON ê°ì²´ ì¶”ì¶œ (ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
    const jsonMatch = cleanedResponse.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      cleanedResponse = jsonMatch[0];
    }
    
    // 4ë‹¨ê³„: JSON ìˆ˜ì • ì‹œë„ (trailing comma ì œê±°)
    cleanedResponse = cleanedResponse.replace(/,(\s*[}\]])/g, '$1');
    
    // 5ë‹¨ê³„: JSON íŒŒì‹± ì‹œë„
    const parsed = JSON.parse(cleanedResponse);
    
    // í•„ìˆ˜ í•„ë“œ ê²€ì¦
    const required = ['expertiseScore', 'personalizedInsight', 'businessHint'];
    for (const field of required) {
      if (!parsed[field]) {
        throw new Error(`Missing required field: ${field}`);
      }
    }

    console.log(`âœ… JSON íŒŒì‹± ì„±ê³µ`);

    return {
      expertiseScore: parseInt(parsed.expertiseScore) || 85,
      personalizedInsight: parsed.personalizedInsight || '',
      businessHint: parsed.businessHint || '',
      marketOpportunity: parsed.marketOpportunity || '',
      successProbability: parsed.successProbability || '85%',
      keyStrengths: parsed.keyStrengths || [],
      nextStepTeaser: parsed.nextStepTeaser || '',
      exclusiveValue: parsed.exclusiveValue || '',
      urgencyFactor: parsed.urgencyFactor || ''
    };
  } catch (error) {
    console.error('âŒ JSON íŒŒì‹± ì‹¤íŒ¨:', {
      error: (error as Error).message,
      originalResponse: llmResponse.substring(0, 200) + (llmResponse.length > 200 ? '...' : ''),
      responseLength: llmResponse.length
    });
    
    console.warn(`ğŸ”„ JSON íŒŒì‹± ì‹¤íŒ¨ - Mockìœ¼ë¡œ í´ë°±`);
    
    return getMockAnalysis(answers);
  }
}

// Mock analysis for fallback
function getMockAnalysis(answers: UserAnswers): AnalysisResult {
  const expertise = answers.expertise_field || answers.expertiseField || 'ì „ë¬¸ ë¶„ì•¼';
  const name = answers.basic_name || answers.name || 'ê³ ê°';
  
  return {
    expertiseScore: 87,
    personalizedInsight: `${name}ë‹˜ì˜ ${expertise} ë¶„ì•¼ ê²½í—˜ì€ AI ê¸°ìˆ ê³¼ ìœµí•©í–ˆì„ ë•Œ ê°•ë ¥í•œ ì°¨ë³„í™” ìš”ì†Œê°€ ë©ë‹ˆë‹¤.`,
    businessHint: `${expertise} Ã— AI ìœµí•© ì„œë¹„ìŠ¤, ê°œì¸ ë§ì¶¤í˜• ì†”ë£¨ì…˜ í”Œë«í¼`,
    marketOpportunity: `${expertise} AI ì‹œì¥ì´ í–¥í›„ 18ê°œì›” ë‚´ ê¸‰ì„±ì¥ ì˜ˆìƒ, ì„ ì  íš¨ê³¼ ê·¹ëŒ€í™” êµ¬ê°„`,
    successProbability: "84% (ìœ ì‚¬ ë°°ê²½ ì „ë¬¸ê°€ 6ê°œì›” í‰ê·  ì„±ê³¼ ê¸°ì¤€)",
    keyStrengths: [
      "ì¥ê¸°ê°„ ì¶•ì ëœ ì „ë¬¸ ì§€ì‹",
      "íƒ€ê²Ÿ ê³ ê°ì¸µ ì´í•´ë„", 
      "ì‹ ë¢° ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬",
      "ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ í†µì°°ë ¥"
    ],
    nextStepTeaser: "êµ¬ì²´ì ì¸ AI ê¸°ìˆ  ìŠ¤íƒ, íŒŒíŠ¸ë„ˆì‚¬ ì—°ê²°, 6ê°œì›” ê°œë°œ ë¡œë“œë§µì€ ì „ë¬¸ê°€ ìƒë‹´ì—ì„œ ë§ì¶¤ ì„¤ê³„í•´ë“œë¦½ë‹ˆë‹¤.",
    exclusiveValue: "ExpertTechë§Œì˜ AI ê°œë°œì‚¬ ë„¤íŠ¸ì›Œí¬ì™€ 200+ ì„±ê³µ ì‚¬ë¡€ ë°ì´í„°ë¡œ 3ê°œì›” ë‚´ MVP ì¶œì‹œ ê°€ëŠ¥",
    urgencyFactor: "í˜„ì¬ ê²½ìŸì‚¬ ì§„ì… ì „ ê³¨ë“ íƒ€ì„, 6ê°œì›” ë‚´ ì‹œì¥ ì„ ì  í•„ìˆ˜"
  };
}

// Main analysis function with fallback chain
async function analyzeExpertise(answers: UserAnswers): Promise<AnalysisResult> {
  const prompt = generateAnalysisPrompt(answers);
  
  try {
    // Primary: GPT-4o
    console.log('ğŸš€ Using GPT-4o (Premium Quality)');
    const response = await callOpenAI(prompt, 'gpt-4o');
    return parseAnalysisResult(response, answers);
  } catch (error) {
    console.error('GPT-4o failed, trying GPT-4o-mini:', error);
    
    try {
      // Fallback: GPT-4o-mini
      console.log('ğŸ”„ Fallback to GPT-4o-mini');
      const response = await callOpenAI(prompt, 'gpt-4o-mini');
      return parseAnalysisResult(response, answers);
    } catch (fallbackError) {
      console.error('All LLM services failed, using intelligent mock:', fallbackError);
      return getMockAnalysis(answers);
    }
  }
}

// Vercel Function handler
export default async function handler(req: VercelRequest, res: VercelResponse) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { interviewData, answers } = req.body;
    const analysisData = interviewData || answers;
    
    if (!analysisData) {
      return res.status(400).json({ 
        success: false, 
        error: 'No interview data provided' 
      });
    }

    console.log('ğŸ“‹ AI ë¶„ì„ ì‹œì‘:', Object.keys(analysisData));
    
    // AI ë¶„ì„ ì‹¤í–‰
    const analysisResult = await analyzeExpertise(analysisData);
    
    console.log('âœ… AI ë¶„ì„ ì™„ë£Œ');
    
    res.status(200).json({
      success: true,
      data: analysisResult,
      message: 'AI ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
    });

  } catch (error) {
    console.error('âŒ AI ë¶„ì„ ì˜¤ë¥˜:', error);
    
    // ì—ëŸ¬ ì‹œì—ë„ Mock ë¶„ì„ ì œê³µ
    const fallbackResult = getMockAnalysis(req.body.interviewData || req.body.answers || {});
    
    res.status(200).json({
      success: true,
      data: fallbackResult,
      message: 'AI ì„œë¹„ìŠ¤ ì¼ì‹œ ì¥ì• ë¡œ ê¸°ë³¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.'
    });
  }
}